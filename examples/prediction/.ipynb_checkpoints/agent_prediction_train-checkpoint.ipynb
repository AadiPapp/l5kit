{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "from avkit.configs import load_config_data\n",
    "from avkit.data import LocalDataManager\n",
    "from avkit.dataset import AgentDataset, EgoDataset\n",
    "from avkit.dataset.utilities import build_dataloader\n",
    "from avkit.rasterization import build_rasterizer\n",
    "from avkit.evaluation import write_coords_as_csv, compute_mse_error_csv\n",
    "from avkit.geometry import transform_points\n",
    "from avkit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `AVKIT_DATA_FOLDER` variable, we can point the script to the folder where the data lie.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasterer, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('format_version', 4), ('model_params', OrderedDict([('model_architecture', 'resnet50'), ('history_num_frames', 0), ('history_step_size', 1), ('history_delta_time', 0.1), ('future_num_frames', 12), ('future_step_size', 1), ('future_delta_time', 0.1)])), ('raster_params', OrderedDict([('raster_size', [224, 224]), ('pixel_size', [0.25, 0.25]), ('ego_center', [0.25, 0.5]), ('map_type', 'py_semantic'), ('satellite_map_key', 'aerial_map/aerial_map.png'), ('semantic_map_lyftbag_key', 'maps/semantic_maps/raster_test_lyftbag.lyftbag'), ('semantic_map_json_key', 'semantic_map/semantic_map.pb'), ('filter_agents_threshold', 0.5)])), ('train_data_loader', OrderedDict([('datasets', [OrderedDict([('key', 'sample_scenes/20200504_competition_sample.zarr'), ('scene_indices', [1])])]), ('perturb_probability', 0.0), ('batch_size', 12), ('shuffle', True), ('num_workers', 16)])), ('val_data_loader', OrderedDict([('datasets', [OrderedDict([('key', 'sample_scenes/20200504_competition_sample.zarr'), ('scene_indices', [0])])]), ('perturb_probability', 0.0), ('batch_size', 12), ('shuffle', False), ('num_workers', 16)])), ('train_params', OrderedDict([('checkpoint_every_n_steps', 10000), ('max_num_steps', 5), ('eval_every_n_steps', 10000)]))])\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"AVKIT_DATA_FOLDER\"] = \"/Users/pondruska/prediction-dataset\"\n",
    "# get config\n",
    "cfg = load_config_data(\"./prediction_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our baseline is a simple `resnet50` pretrained on `imagenet`. We must replace the input and the final layer to address our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet50(pretrained=True)\n",
    "\n",
    "    # change input size\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size\n",
    "    # X, Y  * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device).reshape(len(data[\"target_positions\"]), -1)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss = loss.mean()  # weighted average\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_map/semantic_map.pb is not present in local data folder /tmp/avkit_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "semantic_map/semantic_map.pb not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-046f574a0f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalDataManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ===== INIT DATASETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrasterizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rasterizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgentDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-planning/avkit/avkit/rasterization/factory.py\u001b[0m in \u001b[0;36mbuild_rasterizer\u001b[0;34m(cfg, data_manager)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSatBoxRasterizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mego_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_agents_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msat_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_to_sat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmap_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"py_semantic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msemantic_map_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"semantic_map_json_key\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0msemantic_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_semantic_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_map_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpose_to_ecef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pose_to_ecef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml-planning/avkit/avkit/data/local_data_manager.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} is not present in local data folder {self.root_folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: semantic_map/semantic_map.pb not found"
     ]
    }
   ],
   "source": [
    "dm = LocalDataManager(None)\n",
    "# ===== INIT DATASETS\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_dataloader = build_dataloader(cfg, \"train\", dm, AgentDataset, rasterizer)\n",
    "eval_dataloader = build_dataloader(cfg, \"val\", dm, AgentDataset, rasterizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TRAIN LOOP\n",
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "losses_train = []\n",
    "for _ in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    loss, _ = forward(data, model, device, criterion)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "we can now run inference and store predicted and annotated trajectories. \n",
    "\n",
    "In this example we run it on a single scene from the eval dataset for computationl constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "losses_eval = []\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "future_coords_offsets_gt = []\n",
    "\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "progress_bar = tqdm(eval_dataloader)\n",
    "for data in progress_bar:\n",
    "    loss, ouputs = forward(data, model, device, criterion)\n",
    "    losses_eval.append(loss.item())\n",
    "    progress_bar.set_description(f\"Running EVAL, loss: {loss.item()} loss(avg): {np.mean(losses_eval)}\")\n",
    "\n",
    "    future_coords_offsets_pd.append(ouputs.reshape(len(ouputs), -1, 2).cpu().numpy())\n",
    "    future_coords_offsets_gt.append(data[\"target_positions\"].reshape(len(ouputs), -1, 2).cpu().numpy())\n",
    "\n",
    "    timestamps.append(data[\"timestamp\"].numpy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results in the competition format and perform evaluation\n",
    "After the model has predicted trajectories for our evaluation set, we can save them in a `csv` file in the competiion format. To simulate a complete evaluation session we can also save the GT in another `csv` and get the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== COMPUTE CSV\n",
    "pred_path = f\"{gettempdir()}/pred.csv\"\n",
    "gt_path = f\"{gettempdir()}/gt.csv\"\n",
    "\n",
    "write_coords_as_csv(pred_path, future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                    future_coords_offsets=np.concatenate(future_coords_offsets_pd),\n",
    "                    timestamps=np.concatenate(timestamps),\n",
    "                    agent_ids=np.concatenate(agent_ids))\n",
    "write_coords_as_csv(gt_path, future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                    future_coords_offsets=np.concatenate(future_coords_offsets_gt),\n",
    "                    timestamps=np.concatenate(timestamps),\n",
    "                    agent_ids=np.concatenate(agent_ids))\n",
    "\n",
    "print(f\"current error is {compute_mse_error_csv(gt_path, pred_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise results\n",
    "We can also visualise some result from the ego(AV) point of view. Let's have a look at the frame number `5198`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent_dataset = eval_dataloader.dataset.datasets[0].dataset\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_agent_dataset.dataset, rasterizer)\n",
    "frame_number = 5198\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# get AV point-of-view frame\n",
    "data_ego = eval_ego_dataset[frame_number]\n",
    "im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "\n",
    "\n",
    "center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "agent_indices = eval_agent_dataset.get_frame_indices(frame_number)\n",
    "\n",
    "predicted_positions = []\n",
    "target_positions = []\n",
    "\n",
    "for v_index in agent_indices:\n",
    "    data_agent = eval_agent_dataset[v_index]\n",
    "\n",
    "    out_net = model(torch.from_numpy(data_agent[\"image\"]).unsqueeze(0).to(device))\n",
    "    out_pos = out_net[0].reshape(-1, 2).detach().cpu().numpy()\n",
    "\n",
    "    # store absolute world coordinates\n",
    "    image_to_world = np.linalg.inv(data_agent[\"world_to_image\"])\n",
    "    predicted_positions.append(transform_points(out_pos + center, image_to_world))\n",
    "    target_positions.append(transform_points(data_agent[\"target_positions\"] + center, image_to_world))\n",
    "\n",
    "# convert coordinates to AV point-of-view so we can draw them\n",
    "predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"world_to_image\"]) - center\n",
    "target_positions = transform_points(np.concatenate(target_positions), data_ego[\"world_to_image\"]) - center\n",
    "\n",
    "yaws = np.zeros((len(predicted_positions), 1))\n",
    "draw_trajectory(im_ego, center, predicted_positions, yaws, PREDICTED_POINTS_COLOR)\n",
    "draw_trajectory(im_ego, center, target_positions, yaws, TARGET_POINTS_COLOR)\n",
    "\n",
    "plt.imshow(im_ego[::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
