{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DriveNet Open-Loop Evaluation\n",
    "\n",
    "### what is open-loop evaluation?\n",
    "When we perform open-loop evaluation of our DriveNet model we evaluate its prediction as we follow the annotated ground truth.\n",
    "\n",
    "In each frame, we compare the prediction of our model against the ground truth. This can be done with different metrics, such as ADE (Average Displacement Error) or FDE (Final Displacement Error). \n",
    "\n",
    "**Regardless of the metric used, thie evaluation protocol doesn't modify the future locations according to our predictions**\n",
    "\n",
    "TODO add a picture to explain it maybe?\n",
    "\n",
    "\n",
    "**Note: to generate a model to use here please refer to the training notebook**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, PREDICTED_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.drivenet.model import DriveNetModel\n",
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./drivenet_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\"/var/folders/7f/mb3llzfs40b86yf2km8fml2h0000gp/T/drivenet.pt\").to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=eval_cfg[\"shuffle\"], batch_size=eval_cfg[\"batch_size\"], \n",
    "                             num_workers=eval_cfg[\"num_workers\"])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "position_preds = []\n",
    "yaw_preds = []\n",
    "\n",
    "position_gts = []\n",
    "yaw_gts = []\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "idx = 0\n",
    "\n",
    "for data in tqdm(eval_dataloader):\n",
    "    del data[\"host_id\"]\n",
    "    result = model(data)\n",
    "    position_preds.append(result[\"positions\"].detach().cpu().numpy())\n",
    "    yaw_preds.append(result[\"positions\"].detach().cpu().numpy())\n",
    "\n",
    "    position_gts.append(data[\"target_positions\"].detach().cpu().numpy())\n",
    "    yaw_gts.append(data[\"target_yaws\"].detach().cpu().numpy())\n",
    "    idx += 1\n",
    "    if idx == 10:\n",
    "        break\n",
    "    \n",
    "position_preds = np.concatenate(position_preds)\n",
    "position_gts = np.concatenate(position_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation: ADE and FDE\n",
    "\n",
    "TODO avail\n",
    "TODO yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_errors = np.linalg.norm(position_preds - position_gts, axis=-1)\n",
    "\n",
    "# DISPLACMENT AT T\n",
    "plt.plot(np.arange(pos_errors.shape[1]), pos_errors.mean(0), label=\"Displacement error at T\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ADE HIST\n",
    "plt.hist(pos_errors.mean(-1), bins=100, label=\"ADE Histogram\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# FDE HIST\n",
    "plt.hist(pos_errors[:,-1], bins=100, label=\"FDE Histogram\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Visual Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Results\n",
    "We can also visualise some results from the ego (AV) point of view for those frames of interest (the 100th of each scene).\n",
    "\n",
    "However, as we chopped off the future from the dataset **we must use the GT csv if we want to plot the future trajectories of the agents**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_number in range(0, len(eval_dataset), len(eval_dataset) // 20):\n",
    "    \n",
    "    data = eval_dataloader.dataset[frame_number]\n",
    "    del data[\"host_id\"]\n",
    "\n",
    "    data_batch = default_collate([data])\n",
    "    \n",
    "    result = model(data_batch)\n",
    "    predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "    im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))\n",
    "    target_positions = data[\"target_positions\"]\n",
    "    \n",
    "    predicted_positions = transform_points(predicted_positions, data[\"raster_from_agent\"])\n",
    "    target_positions = transform_points(target_positions, data[\"raster_from_agent\"])\n",
    "    \n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "    \n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.imshow(im_ego[::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
