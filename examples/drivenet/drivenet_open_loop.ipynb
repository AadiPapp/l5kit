{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DriveNet Open Loop Evaluation\n",
    "\n",
    "**Note: this notebook assumes you've already run the [training notebook](./drivenet_train.ipynb) and stored your model successfully.**\n",
    "\n",
    "### What is open loop evaluation?\n",
    "In open-loop evaluation we evaluate our model prediction as we follow the annotated ground truth.\n",
    "\n",
    "In each frame, we compare the predictions of our model against the ones annotated. This can be done with different metrics, such as ADE (Average Displacement Error) or FDE (Final Displacement Error). \n",
    "\n",
    "**Regardless of the metric used, thie evaluation protocol doesn't modify the future locations according to our predictions**\n",
    "\n",
    "\n",
    "TODO add a picture to explain it maybe? With GT, prediction and next step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points, angular_distance\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, PREDICTED_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.drivenet.model import DriveNetModel\n",
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./drivenet_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/var/folders/7f/mb3llzfs40b86yf2km8fml2h0000gp/T/drivenet.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(model_path).to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Data\n",
    "This is almost the exact same code you've already seen in the [training notebook](./drivenet_train.ipynb). Apart from the different dataset we load, the biggest difference is that **we don't perturb our data here**.\n",
    "\n",
    "When performing evaluation we're interested in knowing the performance on the annotated data, not on perturbed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=eval_cfg[\"shuffle\"], batch_size=eval_cfg[\"batch_size\"], \n",
    "                             num_workers=eval_cfg[\"num_workers\"])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "position_preds = []\n",
    "yaw_preds = []\n",
    "\n",
    "position_gts = []\n",
    "yaw_gts = []\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "idx = 0\n",
    "\n",
    "for data in tqdm(eval_dataloader):\n",
    "    del data[\"host_id\"]\n",
    "    result = model(data)\n",
    "    position_preds.append(result[\"positions\"].detach().cpu().numpy())\n",
    "    yaw_preds.append(result[\"yaws\"].detach().cpu().numpy())\n",
    "\n",
    "    position_gts.append(data[\"target_positions\"].detach().cpu().numpy())\n",
    "    yaw_gts.append(data[\"target_yaws\"].detach().cpu().numpy())\n",
    "    idx += 1\n",
    "    if idx == 10:\n",
    "        break\n",
    "    \n",
    "position_preds = np.concatenate(position_preds)\n",
    "yaw_preds = np.concatenate(yaw_preds)\n",
    "\n",
    "position_gts = np.concatenate(position_gts)\n",
    "yaw_gts = np.concatenate(yaw_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation: ADE, FDE and Angle Distance\n",
    "\n",
    "#### Positional Displacement\n",
    "Average Displacement Error (ADE) and Final Displacement Error(FDE) are standard metrics used to evaluate future predictions for AVs.\n",
    "\n",
    "We can easily compute them by comparing predicted and annotated positions, which we have saved in the previous cell.\n",
    "Additionally, we can plot histograms of their distributions across samples to better capture the variance of our error\n",
    "\n",
    "#### Angle Displacement\n",
    "\n",
    "For the yaw, we can use the Minimum Angle Distance to check the error. Again, we can plot an histogram to inspect the error distribution.\n",
    "\n",
    "TODO maybe avail\n",
    "TODO maybe X,Y errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_errors = np.linalg.norm(position_preds - position_gts, axis=-1)\n",
    "\n",
    "# DISPLACEMENT AT T\n",
    "plt.plot(np.arange(pos_errors.shape[1]), pos_errors.mean(0), label=\"Displacement error at T\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ADE HIST\n",
    "plt.hist(pos_errors.mean(-1), bins=100, label=\"ADE Histogram\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# FDE HIST\n",
    "plt.hist(pos_errors[:,-1], bins=100, label=\"FDE Histogram\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "angle_errors = angular_distance(yaw_preds, yaw_gts).squeeze()\n",
    "\n",
    "# ANGLE ERROR AT T\n",
    "plt.plot(np.arange(angle_errors.shape[1]), angle_errors.mean(0), label=\"Angle error at T\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ANGLE ERROR HIST\n",
    "plt.hist(angle_errors.mean(-1), bins=100, label=\"Angle Error Histogram\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Visual Plots\n",
    "\n",
    "### Visualise Results\n",
    "Clearly, manual inspection is still crucial to assess our model performance.\n",
    "\n",
    "We can visualise some images and add predicted and annotated trajectories using L5Kit visualisation features.\n",
    "\n",
    "In this example, we draw 20 images from our dataset and we visualise predicted and annotated trajectories on top of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_number in range(0, len(eval_dataset), len(eval_dataset) // 20):\n",
    "    \n",
    "    data = eval_dataloader.dataset[frame_number]\n",
    "    del data[\"host_id\"]\n",
    "\n",
    "    data_batch = default_collate([data])\n",
    "    \n",
    "    result = model(data_batch)\n",
    "    predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "    im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))\n",
    "    target_positions = data[\"target_positions\"]\n",
    "    \n",
    "    predicted_positions = transform_points(predicted_positions, data[\"raster_from_agent\"])\n",
    "    target_positions = transform_points(target_positions, data[\"raster_from_agent\"])\n",
    "    \n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "    \n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.imshow(im_ego[::-1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Visualise the Open Loop\n",
    "\n",
    "We can also visualise consecutive frames with our DriveNet predictions.\n",
    "\n",
    "In this example, we show the first 200 frames for our dataset, plotting predicted and annotated trajectories.\n",
    "\n",
    "**Note: this is an open loop evaluation, we are NOT controlling the AV with our predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "for frame_number in range(200):\n",
    "    \n",
    "    data = eval_dataloader.dataset[frame_number]\n",
    "    del data[\"host_id\"]\n",
    "\n",
    "    data_batch = default_collate([data])\n",
    "    \n",
    "    result = model(data_batch)\n",
    "    predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "    \n",
    "    predicted_positions = transform_points(predicted_positions, data[\"raster_from_agent\"])\n",
    "    target_positions = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "    \n",
    "    im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))\n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im_ego[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is open loop evaluation enough?\n",
    "\n",
    "Depending on how long you've trained your model, you may have seen different results in the above cell.\n",
    "\n",
    "If you've trained it long enough, the predicted trajectory is likely well overlapped with the annotated one.\n",
    "\n",
    "Conversely, a model trained not enough will surely show some bias or unfeasible trajectories.\n",
    "\n",
    "In both cases, **this evaluation is not enough** to ensure your model will be able to actually drive on the road (that's where we all want to go in the end). If your model is not in controll of where to go, you can't really say it will work once the annotated trajectory won't be available.\n",
    "\n",
    "What we really want to do is to have our model in full control of the AV future positions, in a setting called **close loop**. To this end, you can find a notebook fully dedicated to that TODO HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
