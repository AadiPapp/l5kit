{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DriveNet Close Loop Evaluation\n",
    "\n",
    "**Note: this notebook assumes you've already run the [training notebook](./drivenet_train.ipynb) and stored your model successfully.**\n",
    "\n",
    "## What is a close loop evaluation?\n",
    "In close loop evaluation DriveNet is in **full control of the AV**. At each time step, we predict the future trajectory and then we move the AV int he first of the DriveNet's predictions. \n",
    "\n",
    "At Lyft, we refer to this process with the term **unroll**\n",
    "\n",
    "\n",
    "## What is a good close loop metric?\n",
    "For this setting metrics are particularly challenging. In fact, we would like to penalise some of the drifting (e.g. going off road or in the opposite lane) while at the same time allow others (e.g. different speed profiles)\n",
    "\n",
    "At Lyft L5, we use a substantial sets of different metrics to capture dangerous manoeuvres and behaviours. \n",
    "\n",
    "For the sake of simplicity, in this notebook we will be using a very simple proxy to detect if our model is driving in a sensible way, composed of two different metrics:\n",
    "\n",
    "### Collisions\n",
    "Our AV should avoid collisions with other agents. This sounds trivial, but it's actually more complex than how it looks. In fact, while our AV is fully controlled by Drivenet, other agents are not (a setting we call **log replay**).\n",
    "\n",
    "If our AV was slower than the recorded one, chasing agents might bump into us. Clearly, this won't happen in a real setting where other agents can react to our behaviours.\n",
    "\n",
    "Still, this metric is useful to assess how much our DriveNet follows the rules of the road.\n",
    "\n",
    "However, if we only considered collision, we might under penalise some potentially dangerous situations like driving off-road.\n",
    "\n",
    "### Distance from Reference Trajectory\n",
    "To address the issue presented above, we require our DriveNet to loosely stick to the original trajectory in the data. By setting the right threshold on the distance we can allow for different speed profile and small steerings, while pensalising large deviations like driving off-road.\n",
    "\n",
    "TODO explain what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset, filter_agents_by_frames\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points, angular_distance, yaw_as_rotation33\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, PREDICTED_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.drivenet.model import DriveNetModel\n",
    "from l5kit.drivenet.utils import detect_collision\n",
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./drivenet_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/drivenet.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(model_path).to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Data\n",
    "Differently from training and open loop evaluation, this setting is intrinsically sequential. As such, we won't be using any parallelisation offered by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Unroll function\n",
    "\n",
    "Our unroll function is actually very simple. At each timestep:\n",
    "- we **forward the current frame** to DriveNet;\n",
    "- we **get the predicted trajectory** from the model;\n",
    "- we **compute our metrics** (collisions and distance from the reference trajectory);\n",
    "- if we have not collided with other agents and we're still on the road we can use the first point of the trajectory as position for the next frame;\n",
    "- otherwise, we just reset the AV to the original GT position.\n",
    "\n",
    "The function returns not only the RGB frames, but also the different types of errors according to the metrics.\n",
    "\n",
    "After this function is defined we can iterate through scenes, visualise the result and plot the accumulated metrics very easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_scene(scene_dataset, model, drifting_threshold=10):\n",
    "    ## prepare return buffers\n",
    "    images = []\n",
    "    collisions = []\n",
    "    driftings = []\n",
    "\n",
    "    for frame_idx in tqdm(range(len(scene_dataset))):\n",
    "        data = scene_dataset[frame_idx]\n",
    "        del data[\"host_id\"]\n",
    "        data_batch = default_collate([data])\n",
    "        result = model(data_batch)\n",
    "        \n",
    "        predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "        predicted_yaws = result[\"yaws\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "        ## store image\n",
    "        im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))    \n",
    "        draw_trajectory(im_ego, transform_points(predicted_positions, data[\"raster_from_agent\"]), PREDICTED_POINTS_COLOR)\n",
    "        images.append(im_ego[::-1])\n",
    "\n",
    "        ## compute absolute positions\n",
    "        pred_positions_m = transform_points(predicted_positions, data[\"world_from_agent\"])\n",
    "        pred_angles_rad = predicted_yaws + data[\"yaw\"]\n",
    "        gt_positions_m = transform_points(data[\"target_positions\"], data[\"world_from_agent\"])\n",
    "\n",
    "        ## detect collisions\n",
    "        agents_frame = filter_agents_by_frames(scene_dataset.dataset.frames[frame_idx], scene_dataset.dataset.agents)[0]\n",
    "        collision = detect_collision(data[\"centroid\"], data[\"yaw\"], data[\"extent\"],agents_frame)\n",
    "        collisions.append(collision)\n",
    "        if collision[0] != \"\":\n",
    "            continue\n",
    "\n",
    "        ## detect drifting\n",
    "        drifting = np.linalg.norm(pred_positions_m[0] - gt_positions_m[0]) > drifting_threshold\n",
    "        driftings.append(drifting)\n",
    "        if drifting:\n",
    "            continue\n",
    "\n",
    "        ## mutate the next frame if we're not at the end of the scene\n",
    "        frame_mutate_idx = frame_idx + 1\n",
    "        if frame_mutate_idx < len(scene_dataset):\n",
    "            scene_dataset.dataset.frames[frame_mutate_idx][\"ego_translation\"][:2] = pred_positions_m[0]\n",
    "            scene_dataset.dataset.frames[frame_mutate_idx][\"ego_rotation\"] = yaw_as_rotation33(pred_angles_rad[0])\n",
    "    \n",
    "    return images, collisions, driftings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our function on a scene\n",
    "In this cell, we test our unroll function on a scene from the evaluation set. \n",
    "\n",
    "L5Kit comes with a handy function to separate an individual scene from a bigger dataset that we can use to unroll a single scene (or a set) of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "scenes_to_unroll = 20\n",
    "images, collisions, driftings = [], [], []\n",
    "for scene_idx in range(0, len(eval_zarr.scenes), len(eval_zarr.scenes)//scene_to_unroll):\n",
    "    scene_dataset = eval_dataset.get_scene_dataset(scene_idx)\n",
    "    scene_images, scene_collisions, scene_driftings = unroll_scene(scene_dataset, model)\n",
    "    images.append(scene_images)\n",
    "    collisions.append(scene_collisions)\n",
    "    driftings.append(scene_driftings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Visualise the Close Loop\n",
    "\n",
    "We can visualise the frames we have stored in the previous cell. \n",
    "\n",
    "**DriveNet is now in full control of the AV as it moves through the annotated scene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    "from time import sleep\n",
    " \n",
    "for frame in images[2]:\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(frame))\n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation: Plotting Errors from the Close Loop\n",
    "\n",
    "We can collect metrics from different scenes and plot them here.\n",
    "\n",
    "For collision, we have split them into *rear, front and side* to better capture the nature of different errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_names = np.asarray([collision[0] for scene_collisions in collisions for collision in scene_collisions])\n",
    "drifrings = np.asarray([drift for scene_driftings in driftings for drift in scene_driftings])\n",
    "\n",
    "values = []\n",
    "names = []\n",
    "\n",
    "for collision_name in [\"front\", \"side\", \"rear\"]:\n",
    "    values.append(np.sum(collision_names == collision_name))\n",
    "    names.append(collision_name)\n",
    "\n",
    "values.append(np.sum(driftings == True))\n",
    "names.append(\"displacement error\")\n",
    "\n",
    "plt.bar(np.arange(len(names)), values)\n",
    "plt.xticks(np.arange(len(names)), names)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
