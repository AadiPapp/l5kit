{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DriveNet Close Loop Evaluation\n",
    "\n",
    "**Note: this notebook assumes you've already run the [training notebook](./drivenet_train.ipynb) and stored your model successfully.**\n",
    "\n",
    "### What is close loop evaluation?\n",
    "In close loop evaluation DriveNet is in **full control of the AV**. At each time step, we predict the future trajectory and then we move the AV int he first of the DriveNet's predictions. \n",
    "\n",
    "For this setting metrics are particularly challenging. In fact, we would like to penalise some of the drifting (e.g. going off road or in the opposite lane) while at the same time allow others (e.g. different speed profiles)\n",
    "\n",
    "Internally, we use a substantial sets of different metrics to capture dangerous manoeuvres and behaviours. For the sake of simplicity, in this notebook we will be using a very simple proxy to detect if our model is driving in a sensible way, **collisions with other agents**.\n",
    "\n",
    "Although simple, when used in densily crowded area this metric is surprisingly effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points, angular_distance, yaw_as_rotation33\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, PREDICTED_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.drivenet.model import DriveNetModel\n",
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./drivenet_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/drivenet.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(model_path).to(device)\n",
    "model = model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Evaluation Data\n",
    "Differently from training and open loop evaluation, this setting is intrinsically sequential. As such, we won't be using any parallelisation offered by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unroll the first scene\n",
    "\n",
    "in this cell we unroll the first scene of the dataset\n",
    "\n",
    "TODO generic function for this\n",
    "\n",
    "TODO add collisions metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "scene_dataset = eval_dataset.get_scene_dataset(0)\n",
    "\n",
    "images = []\n",
    "\n",
    "for frame_idx in tqdm(range(len(scene_dataset))):\n",
    "    data = scene_dataset[frame_idx]\n",
    "    del data[\"host_id\"]\n",
    "    data_batch = default_collate([data])\n",
    "    result = model(data_batch)\n",
    "    predicted_positions = result[\"positions\"].detach().cpu().numpy().squeeze()\n",
    "    predicted_yaws = result[\"yaws\"].detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    ## store image for future plot\n",
    "    im_ego = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))    \n",
    "    draw_trajectory(im_ego, transform_points(predicted_positions, data[\"raster_from_agent\"]), PREDICTED_POINTS_COLOR)\n",
    "    images.append(im_ego[::-1])    \n",
    "    \n",
    "    ## mutate the next frame\n",
    "    pred_positions_m = transform_points(predicted_positions, data[\"world_from_agent\"])\n",
    "    pred_angles_rad = predicted_yaws + data[\"yaw\"]\n",
    "\n",
    "    frame_mutate_idx = frame_idx + 1\n",
    "    if frame_mutate_idx < len(scene_dataset):\n",
    "        scene_dataset.dataset.frames[frame_mutate_idx][\"ego_translation\"][:2] = pred_positions_m[0]\n",
    "        scene_dataset.dataset.frames[frame_mutate_idx][\"ego_rotation\"] = yaw_as_rotation33(pred_angles_rad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: Visualise the Close Loop\n",
    "\n",
    "We can visualise the frames we have stored in the previous cell. As we have mutated future positions, DriveNet is now in full control of the AV as it moves through the annotated scene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "for frame in images:\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(frame))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
